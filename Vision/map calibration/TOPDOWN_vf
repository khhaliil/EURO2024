import cv2
import cv2.aruco as aruco
import numpy as np
from save_data import save_frame


# Load calibration data from .npz file
calib_data_path = r"Vision\calibration\Calib_matrix\MultiMatrixXiaomi.npz"
calib_data = np.load(calib_data_path)
cameraMatrix = calib_data['camMatrix']
distCoeffs = calib_data['distCoef']

global capture_mode
capture_mode = False
# Set the mouse callback function for your top_down_view window
cv2.namedWindow('Top-Down View')
def click_event(event, x, y, flags, param):
    global capture_mode
    if event == cv2.EVENT_LBUTTONDOWN and capture_mode:
        captured_x, captured_y = x, y
        #real_x, real_y = pixel_to_real(captured_x, captured_y, scale_factor, x_offset, y_offset)
        print(f"Captured pixel coordinates: {captured_x}, {captured_y}")
        #print(f"Converted real-world coordinates: {real_x} mm, {real_y} mm")
        capture_mode = False

cv2.setMouseCallback('Top-Down View', click_event)

###########################  PIXEL TO REAL ##########################################

# def pixel_to_real(x_pixel, y_pixel, scale_factor, x_offset, y_offset):
#     """
#     Convert pixel coordinates to real-world coordinates.

#     Args:
#     - x_pixel, y_pixel: Pixel coordinates to be converted.
#     - scale_factor: Scale factor for conversion (real-world units per pixel).
#     - x_offset, y_offset: Offsets in real-world units.

#     Returns:
#     - (x_real, y_real): Converted real-world coordinates.
#     """
#     x_real = x_pixel * scale_factor + x_offset
#     y_real = y_pixel * scale_factor + y_offset  # Assuming the same scale for y if needed
#     return x_real, y_real

# # Reference points
# point1_pixel = (163, 120)
# point1_real = (750, 1500)
# point2_pixel = (485, 120)
# point2_real = (2250, 1500)

# # Calculate the scale factor and offsets
# scale_factor = (point2_real[0] - point1_real[0]) / (point2_pixel[0] - point1_pixel[0])
# x_offset = point1_real[0] - (point1_pixel[0] * scale_factor)
# y_offset = point1_real[1] - (point1_pixel[1] * scale_factor)  # Assuming the same scale for y if needed

#####################################################################

# Load your image
# img_path = r"Vision\test_images\WhatsApp Image 2023-12-08 at 17.28.20.jpeg"
# img = cv2.imread(img_path)

# siamrpn tracking 
# cv2 tracking

# Undistort the image
#img_undistorted = cv2.undistort(img, cameraMatrix, distCoeffs, None, cameraMatrix)
def rotate_image(image, angle):
    """
    Rotates an image by the given angle.

    Args:
    - image: Image to be rotated.
    - angle: Angle in degrees to rotate the image. Positive values mean
             counter-clockwise rotation.

    Returns:
    - Rotated image.
    """
    # Get the image dimensions
    height, width = image.shape[:2]
    # Calculate the center of the image
    center = (width / 2, height / 2)
    # Get the rotation matrix
    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1)
    # Perform the rotation
    rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))
    return rotated_image
# img = rotate_image(img,-1)
# ArUco marker parameters
marker_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)
aruco_params = aruco.DetectorParameters()
# marker_length = 0.1 # Marker length in meters
# x_aruco = 0.517
# y_aruco= 0.755
mapx = 2
mapy = 3
cap = cv2.VideoCapture(2)
cv2.namedWindow("Trackbars")

def nothing(x):
    pass
# Create trackbars for adjusting parameters
cv2.createTrackbar("Marker Length", "Trackbars", 10, 20, nothing)  # Range from 0.1m to 0.2m
cv2.createTrackbar("X Aruco", "Trackbars", 517, 1000, nothing)  # Example range
cv2.createTrackbar("Y Aruco", "Trackbars", 755, 1000, nothing)
while True : 

    ret,img = cap.read()
    img = cv2.flip(img, -1)
# Detect ArUco marker   s
    if not ret :
        break 
    corners, ids, rejected = aruco.detectMarkers(img, marker_dict, parameters=aruco_params)
# Define marker locations in 3D space (assuming they lie on the z=0 plane)
    marker_length = cv2.getTrackbarPos("Marker Length", "Trackbars") / 100.0  # Scaling back to meters
    x_aruco = cv2.getTrackbarPos("X Aruco", "Trackbars") / 1000.0  # Adjust scale as needed
    y_aruco = cv2.getTrackbarPos("Y Aruco", "Trackbars") / 1000.0  # Adjust scale as needed
    marker_locations_3d = {
    21: np.array([(x_aruco-marker_length, y_aruco-marker_length, 0), (x_aruco+marker_length, y_aruco-marker_length, 0), (x_aruco+marker_length, y_aruco+marker_length, 0), (x_aruco-marker_length, y_aruco+marker_length, 0)]),
    23: np.array([(mapx-x_aruco-marker_length, y_aruco-marker_length, 0), (mapx-x_aruco+marker_length, y_aruco-marker_length, 0), (mapx-x_aruco+marker_length, y_aruco+marker_length, 0), (mapx-x_aruco-marker_length, y_aruco+marker_length, 0)]),
    20: np.array([(x_aruco-marker_length, mapy-y_aruco-marker_length, 0), (x_aruco+marker_length, mapy-y_aruco-marker_length, 0), (x_aruco+marker_length, mapy-y_aruco+marker_length, 0), (x_aruco, mapy-y_aruco+marker_length, 0)]),
    22: np.array([(mapx-x_aruco-marker_length, mapy-y_aruco-marker_length, 0), (mapx-x_aruco+marker_length, mapy-y_aruco-marker_length, 0), (mapx-x_aruco+marker_length, mapy-y_aruco+marker_length, 0), (mapx-x_aruco-marker_length, mapy-y_aruco+marker_length, 0)])
    }   
# Assume all markers are on the same flat surface (z=0)
    image_points = []
    object_points = []
    if ids is not None:
        for i, marker_id in enumerate(ids.flatten()):
            if marker_id in marker_locations_3d:
                image_points.append(corners[i][0])
                object_points.append(marker_locations_3d[marker_id])
# Flatten the arrays for homography
    image_points = np.array(image_points).reshape(-1, 2)
    object_points = np.array(object_points).reshape(-1, 3)
# Compute the homography matrix if we have enough correspondences
    if len(image_points) >= 4:
    # Calculate homography
        homography_matrix, _ = cv2.findHomography(object_points[:, :2], image_points)
    # Define the 3D coordinates of the map corners assuming z=0
        map_corners_3d = np.float32([[0, 0], [2, 0], [2, 3], [0, 3]])
    # Transform the map corners using the homography matrix
        map_corners_2d = cv2.perspectiveTransform(map_corners_3d.reshape(-1, 1, 2), homography_matrix)
        #print (map_corners_2d)
    # Draw the projected corners on the image
        points = []
        for i in range(map_corners_2d.shape[0]):
            point = tuple(map_corners_2d[i, 0].astype(int))
            points.append(point)
        #cv2.circle(img, point, 10, (0, 255, 0), -1)
        #cv2.putText(img,str(i),point,cv2.FONT_HERSHEY_COMPLEX,1,(0, 255, 0),2)
        #print(img.shape)
        src_pts = np.array([points[1],points[0],  points[2], points[3]], dtype='float32')
        dst_pts = np.array([[0,0],[ 0,img.shape[0]],  [img.shape[1], 0], [ img.shape[1],img.shape[0]]], dtype='float32')
        M = cv2.getPerspectiveTransform(src_pts, dst_pts)
 # Warp the image to get the top-down view
        img_top_down = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]))
    #img_top_down=cv2.cvtColor(img_top_down,cv2.COLOR_BGR2GRAY)
        cv2.imshow('Top-Down View', img_top_down)
        cv2.imshow('Marked Image', img)


        #print(img_top_down.shape)
       

        key = cv2.waitKey(1) & 0xFF
        if key == ord('s'):
        # Manual save
            save_frame(img_top_down, prefix='top_down', manual_save=True)
        else:
        # Automatic save every 10 seconds
            save_frame(img_top_down, prefix='top_down', interval=8)


        if key == ord('g'):
            # Toggle capture mode
            capture_mode = not capture_mode
            if capture_mode:
                print("Capture mode activated. Click on the desired pixel.")
            else:
                print("Capture mode deactivated.")
        
    else:
        cv2.imshow('Marked Image', img)
        print("Not enough markers detected to compute the homography matrix.")
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
cap.release()
cv2.destroyAllWindows()
